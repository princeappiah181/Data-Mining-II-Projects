---
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    number_sections: true
    toc: true
    toc_depth: 4
title: "Project 2: Optimization and Kernel Trick"
author: 
- Appiah Prince^[pappiah@miners.utep.edu]
- University of Texas at El Paso (UTEP) 
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontsize: 12pt
spacing: single
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \rhead{STAT 5494-- Statistical Machine Learning}
- \lhead{Optimization and Kernel Trick}
- \cfoot{\thepage}
- \usepackage{algorithm}
- \usepackage[noend]{algpseudocode}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Bring in the data
```{r}
data <- read.csv('Shill Bidding Dataset.csv')
names(data)

# Remove the first three columns
shill_bidding <- data[, -c(1:3)] 
names(shill_bidding)

#change the 0 value of the class variable to -1
shill_bidding$Class[shill_bidding$Class == 0] <- -1
#shill_bidding$Class <- ifelse(shill_bidding$Class == 0, -1, 1) 
table(shill_bidding$Class)
dim(shill_bidding)
```

\
Remarks

- The data Shill Bidding was loaded and the first three columns were remove
  since these were ID variables.
- The original dimension of the data was 6321 rows and 13 columns.
  However, since we removed three columns, our new dimension is 6,321 rows and
  10 columns.
- I printed a table of the target variable(class) to confirm that the level 0 
  has been changed to -1.
  

# Exploratory Data Analysis (EDA)
## Distinct levels or values for each variable
```{r}
aggregate(values ~ ind, unique(stack(shill_bidding)), length)
str(shill_bidding)
```


\
Remarks

The numerical variables **class** and **Successive_Outbidding** have only few
distinct values.

##  Missing Values
```{r}
library(questionr)
freq.na(shill_bidding)
```

\
Remarks

There are no missing values in the data

## Parallel Boxplot of the Data
```{r}
boxplot(shill_bidding[,-10], col = rainbow(ncol(shill_bidding[,-10])))
```


\
Remarks

- The predictors have unequal range and unequal variation. In particular, the  
predictors **Auction_Duration**,**Starting_Price_Average **,**Winning_Ratio** 
and **Successive_Outbidding** have notable unequal range and variation.
- Hence,scaling is necessary for some modeling approaches.


## Bar plot of the binary response Class
```{r}
library(ggplot2)
library(dplyr)

shill_bidding %>% 
    count(class = factor(Class)) %>% 
    mutate(pct = prop.table(n)) %>% 
    ggplot(aes(x = class, y = pct, label = scales::percent(pct), fill=class)) + 
    geom_col(position = 'dodge') + 
    geom_text(position = position_dodge(width = .9),  
              vjust = -0.5,    
              size = 3) + 
    scale_y_continuous(labels = scales::percent) +
    theme(legend.position = "none")
```

\
Remarks

We see from the barplot that the percentage of 1's is 11% and percentage of -1's is 89%. Therefore, we do not have an unbalanced classification problem.



#  Data Partitioning
```{r}
set.seed(125)
Data <- sample(seq(1, 3), size = nrow(shill_bidding), replace = TRUE, 
              prob = c(0.5, 0.25, 0.25))
train_data <- shill_bidding[Data == 1, ] # training data
validation_data <- shill_bidding[Data == 2, ] # validation data
test_data <- shill_bidding[Data == 3, ] # test data

dim(train_data)
dim(validation_data)
dim(test_data)
```

\
Remarks

- There are `r dim(train_data)[1]` observations  and `r dim(train_data)[2]` 
variables in the training data.
- There are `r dim(validation_data)[1]` observations and 
`r dim(validation_data)[2]` variables in the validation data.
- There are `r dim(test_data)[1]` observations and `r dim(test_data)[2]`
variables in the test data.



# Logistic Regression - Optimization

- Part 4(a)

## Pool the training data and the validation data together
```{r}
train_valid_data <- rbind(train_data, validation_data)
#head(train_valid_data)
dim(train_valid_data)
```

## Negative Likelihood Function and Test On the shill_bidding data
```{r}
# THE NEGATIVE LOGLIKEHOOD FUNCTION FOR Y=+1/-1
nloglik <- function(beta, X, y){
	if (length(unique(y)) !=2) stop("Are you sure you've got Binary Target?") 
	X <- cbind(1, X)
	nloglik <- sum(log(1+ exp(-y*X%*%beta)))
	return(nloglik) 
}

y <- train_valid_data$Class
X <- as.matrix(train_valid_data[, c(1:9)])
p <- NCOL(X) +1
fit <- optim(par=rep(0,p), fn=nloglik, method="BFGS", X=X, y=y, 
              hessian = TRUE)
beta.hat <- fit$par # obtaining the regression parameters
beta.hat
```
\
Remarks

The optimization method that was employed in R function optim() is BFGS


## Standard error from the Hessian matrix
```{r}
hessian <- fit$hessian # Hessian matrix
inv_hessian <- solve(hessian)
standard_error <- sqrt(diag(inv_hessian))
standard_error
```



## Convergence of the algorithm
```{r}
fit$convergence
```
 
 \ 
Remarks
The algorithm converges since the output of fit$convergence is 0.


## Testing the significance of each attribute and table of results 
```{r}
p0 <- length(beta.hat)-1
z.wald <- beta.hat/standard_error
pvalue <- pchisq(z.wald^2, df=1, lower.tail=FALSE)
result <- data.frame(beta.hat, standard_error, z.wald, pvalue)
row.names(result) <- c("Intercept", names(shill_bidding[, -10]))
round(result, digits = 4)
```

\
Remarks

- Taken $\alpha = 0.05$ as a threshold, we see from our results that the p-values
for the predictors **Bidder_Tendency**, **Successive_Outbidding** and  
**Winning_Ratio** are less that $\alpha = 0.05$. Hence, these attributes are 
statistically significant.
 


- Part (4b)

## Comparing results in 4(a) with fitting results from glm()
```{r}
y <- factor(train_valid_data$Class)
fit.logit <- glm(y~Bidder_Tendency+Bidding_Ratio+Successive_Outbidding+
                   Last_Bidding+Auction_Bids+Starting_Price_Average+
                   Early_Bidding+Winning_Ratio+Auction_Duration,
                 data=train_valid_data, family=binomial(link = "logit"))
result <- summary(fit.logit)
round(result$coefficients, 4)
fit.logit$converged
```


\

Remarks

- The glm() result also converges.

- Taken $\alpha = 0.05$ as a threshold, we see from our results that the p-values
for the attributes **Bidder_Tendency**, **Successive_Outbidding** and 
**Winning_Ratio** are less that $\alpha = 0.05$. Hence, these attributes are
statistically significant.

- The coefficient of Winning_Ratio says that, holding the other predictors 
at a fixed value, we will see $e^{4.7765} = 11868.82\%$ increase in the odds of 
getting into a positive Class for a unit increase in Winning_Ratio.

- There appears to be no difference between the results of the two methods.


- Part 4(c)

## Making prediction using the test data
```{r}
my_fun <- function(x){
  exp(x)/(1+exp(x) )
}

test_data <- test_data
new_X <- as.matrix(cbind(1,test_data[,-10]))
y_hat_prime <- sign(my_fun(new_X%*%fit$par) - 0.5)
conf_matt <- table(test_data$Class, y_hat_prime) # gives the confusion matrix
#round(mean(test_data$Class == y_hat_prime),) 

pred_acc <- sum(diag(conf_matt))/sum(conf_matt) # gives the accuracy
pred_acc

```


\

Remarks

With a threshold of 0.5, our prediction accuracy is 0.9787645. This means, the 
algorithm in 4(a) predicts or models the data very well.


# Primitive LDA (The Kernel Trick ) 

## Matrix of all predictors and Scaling X1 and X2

- Part 5(a)
```{r}
# matrix of all predictors for the three train, validation and test sets
X1 <- as.matrix(train_data[,-10])
X2 <- as.matrix(validation_data[,-10])
X3 <- as.matrix(test_data[,-10])

#scale X1
X1_scale <- scale(X1, center = TRUE, scale = TRUE)

# scale X2 according to the column means and SDs computed from X1
mean_X1 <- attributes(X1_scale)$'scaled:center' 
sd_X1 <- attributes(X1_scale)$'scaled:scale' 
X2_scale <- scale(X2, center = mean_X1, scale = sd_X1)
```



## Train the primitive LDA classifier with D1 and use the prediction accuracy on D2

- Part 5(b)
```{r}
library(kernlab)

LDA_P <- function (kernel, X, Y=NULL, target) {
    kernmat <- kernelMatrix
    w.z <- colMeans(kernmat(kernel, x=X[target==1,], y=Y)) -
             colMeans(kernmat(kernel, x=X[target==-1,], y=Y)) 
    b <- (mean(kernmat(kernel, X[target==-1,])) -
    mean(kernmat(kernel, X[target==1,])))*.5 
    yhat <- sign(w.z + b)
    return(yhat)
}

deg_vec <- 1:15
pred.acc_vec <- rep(0, length(deg_vec))
for (i in 1:length(deg_vec)) {
    d <- deg_vec[i]
     kern <- polydot(degree = d, offset = 1, scale = 1)
  
    #compute prediction accuracy
    ypred <- LDA_P(kern, X1_scale, X2_scale, train_data$Class)
    yobserved <- validation_data$Class
    conf_mat <- table(ypred, yobserved)
    pred_accuracy <- sum(diag(conf_mat))/sum(conf_mat)
    pred.acc_vec[i] <- pred_accuracy
    
}
 plot(deg_vec, pred.acc_vec, type = "b", col="red")
 max(pred.acc_vec); min(pred.acc_vec)
```


\

Remarks

- The kernel family used was polynomial kernel.
- From the plot of the prediction accuracy values versus the candidate parameter values,the best choice of our parameter(degree) is 2. 
- The maximum value of the prediction accuracy is **0.9655388**. Thus, we see 
that the polynomial kernel helps well in the classification.
- The minimum value of the prediction accuracy is **0.8997494**.



## Apply the trained classifier with the `best' kernel found in 5(b) to the test data D3.

- Part 5(c)
```{r}
# Scale X.prime
X_prime <- as.matrix(train_valid_data[, -10])
X_prime_scale <- scale(X_prime, center = TRUE,   scale = TRUE)

# Scale X3 according to the column means and SDs computed from X.prime
mean_X_prime_scale <- attributes(X_prime_scale)$'scaled:center' 
sd_X_prime_scale <- attributes(X_prime_scale)$'scaled:scale' 
X3_scale <- scale(X3, center = mean_X_prime_scale , scale = sd_X_prime_scale)
```


```{r}
# Apply the best kernel to the test data
kern <- polydot(degree = which.max(pred.acc_vec))
    
    #compute prediction accuracy
    ypred <- LDA_P(kern, X_prime_scale, X3_scale, train_valid_data$Class)
    yobserved <- test_data$Class
    conf_mat <- table(ypred, yobserved)
    pred_accuracy_test <- sum(diag(conf_mat))/sum(conf_mat)
    pred_accuracy_test
```

\

Remarks

- The prediction accuracy obtained after applying the trained classifier with
the 'best' kernel found in 5(b) to the test data is **`r pred_accuracy_test`**.
This shows an excellent prediction or classification ability of the model.



## Comparison of the prediction accuracy obtained in 4(c) and 5(c)
```{r}
comparison <- data.frame(pred_accuracy_test,pred_acc )
knitr::kable(comparison, 
        col.names = c("Primitive LDA", "Logistic regression (optimization)"), 
        caption = "Prediction accuracy", align = "cc")
```


\

Remarks

There is a small difference between the prediction accuracy obtained in 4(c) and 5(c)






